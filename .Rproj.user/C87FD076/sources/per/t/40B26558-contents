#' Function for estimating a Poisson-Lindley regression model
#'
#' @name poisLind
#' @param formula an R formula.
#' @param method a method to use for optimization in the maximum likelihood estimation. For options, see \code{\link[maxLik]{maxLik}},
#' @param data a dataframe that has all of the variables in the \code{formula}.
#' @param max.iters the maximum number of iterations to allow the optimization method to perform,
#' @param print.level determines the level of verbosity for printing details of the optimization as it is computed. A value of 0 does not print out any information, a value of 1 prints minimal information, and a value of 2 prints the most information.
#'
#'
#' @import nlme  maxLik  MASS  stats modelr
#' @include plind.R
#'
#' @details
#' The Poisson-Lindley regression is based on a compound Poisson-Lindley distribution. It handles count outcomes with high levels of zero observations (or other high densities at low outcome values) that standard count regression methods, including the negative binomial, may struggle to adequately capture or model.
#'
#' The compound Probability Mass Function(PMF) for the Poisson-Lindley (PL) distribution is:
#' \deqn{f(y|\theta,\lambda)=\frac{\theta^2\lambda^y(\theta+\lambda+ y+1)}{(\theta+1)(\theta+\lambda)^{y+2}}}
#'
#' Where \eqn{\theta} and \eqn{\lambda} are distribution parameters with the restrictions that \eqn{\theta>0} and \eqn{\lambda>0}, and \eqn{y} is a non-negative integer.
#'
#' The expected value of the distribution is:
#' \deqn{\mu=\frac{\lambda(\theta+2)}{\theta(\theta+1)}}
#'
#' If a log-link function is used, the mean is:
#' \deqn{\mu=e^{X\beta}=\frac{\lambda(\theta+2)}{\theta(\theta+1)}}
#'
#' Thus, the parameter \eqn{\lambda} in the PL distribution when applied to regression analysis is:
#' \deqn{\lambda=\frac{\mu\theta(\theta+1)}{\theta+2}}
#'
#' The variance function is defined as:#'
#' \deqn{\sigma^2=\mu+\left(1-\frac{2}{(\theta+2)^2}\right)\mu^2}
#'
#' It should be noted that the p-value for the parameter `ln(theta)` in the model summary is testing if the parameter `theta` is equal to a value of 1. This has no practical meaning. The Likelihood-Ratio (LR) test compares the Poisson-Lindley regression with a Poisson regression with the same independent variables. Thus, the PR test result indicates the statistical significance for the improvement in how well the model fits the data over a Poisson regression. This indicates the statistical significance of the `theta` parameter.
#'
#'
#' @examples
#' # Poisson-Lindley Model
#' data("washington_roads")
#' washington_roads$AADTover10k <- ifelse(washington_roads$AADT>10000,1,0) # create a dummy variable
#' poislind.mod <- poisLind(Animal ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k,
#'                                 data=washington_roads,
#'                                 max.iters = 1000)
#' summary(poislind.mod)
#' @export
poisLind <- function(formula, data, method = 'BHHH', max.iters = 1000, print.level=0) {

  mod_df <- stats::model.frame(formula, data)
  X <- as.matrix(modelr::model_matrix(data, formula))
  y <- as.numeric(stats::model.response(mod_df))
  x_names <- colnames(X)
  n.obs <- length(y)
  n.coefs <- ncol(X)

  # Use the Negative Binomial as starting values
  p_model <- MASS::glm.nb(formula, data = data)
  start <- unlist(p_model$coefficients)
  t <- 116761/exp(-9.04*p_model$theta)
  theta <- ifelse(t<100,t,1) # Approximate Initial Theta

  full_start <- append(start, log(theta))

  modparams <- as.numeric(length(full_start))
  
  # Define gradient and Hessian functions
  gradFun <- function(beta, y, X) {
    grad <- matrix(0, nrow = n.obs, ncol = length(beta))
    
    coefs <- beta[1:n.coefs]
    ln_theta <- unlist(beta[length(beta)])
    theta <- exp(ln_theta)
    
    predicted <- exp(X %*% coefs)
    mu <- predicted
    
    for (i in 1:n.obs) {
      x_i <- X[i, ]
      mu_i <- mu[i]
      y_i <- y[i]
      
      grad_beta <- (y_i - mu_i) * x_i
      grad_theta <- ((2 * theta * (y_i + 1) - 2 * theta^2 - theta^2 * y_i) / (theta * (theta + y_i + 1))) - 1
      grad_theta <- grad_theta / theta
      
      grad[i, ] <- c(grad_beta, grad_theta)
    }
    
    return(grad)
  }
  
  # hessFun <- function(beta, y, X) {
  #   hess <- matrix(0, nrow = length(beta), ncol = length(beta))
  #   
  #   coefs <- beta[1:n.coefs]
  #   ln_theta <- unlist(beta[length(beta)])
  #   theta <- exp(ln_theta)
  #   
  #   predicted <- exp(X %*% coefs)
  #   mu <- predicted
  #   
  #   for (i in seq_along(y)) {
  #     x_i <- X[i, ]
  #     mu_i <- mu[i]
  #     y_i <- y[i]
  #     
  #     hess_beta_beta <- -mu_i * (x_i %*% t(x_i))
  #     hess_theta_theta <- -mu_i * (trigamma(theta + 1) - trigamma(1)) / ((digamma(theta + 1) - digamma(1))^2)
  #     hess_beta_theta <- -mu_i * x_i * (trigamma(theta + 1) - trigamma(1)) / ((digamma(theta + 1) - digamma(1))^2)
  #     
  #     hess[1:n.coefs, 1:n.coefs] <- hess[1:n.coefs, 1:n.coefs] + hess_beta_beta
  #     hess[length(beta), length(beta)] <- hess[length(beta), length(beta)] + hess_theta_theta
  #     hess[1:n.coefs, length(beta)] <- hess[1:n.coefs, length(beta)] + hess_beta_theta
  #     hess[length(beta), 1:n.coefs] <- hess[length(beta), 1:n.coefs] + t(hess_beta_theta)
  #   }
  #   
  #   return(hess)
  # }
  

  logLikFun <- function(beta, y, X) {
    pars <- length(beta) - 1
    coefs <- as.vector(unlist(beta[1:pars]))
    theta <- exp(unlist(beta[length(beta)]))
    
    predicted <- exp(X %*% coefs)
    probs <- dplind(y, mean = predicted, theta = theta)
    ll <- sum(log(probs))
    
    if (method == 'bhhh' || method == 'BHHH') {
      log_probs <- log(probs)
      return(log_probs)
    } else {
      return(ll)
    }
  }

  fit <- maxLik::maxLik(
    logLik = logLikFun,
    grad = if (method == 'BHHH') gradFun else function(...) colSums(gradFun(...)),
    hess = NULL,
    start = full_start,
    y = y,
    X = X,
    method = method,
    control = list(iterlim = max.iters, printLevel = print.level)
  )

  beta_est <- fit$estimate
  npars <- length(beta_est)-1

  beta_pred <- as.vector(unlist(beta_est[1:npars]))
  fit$beta_pred <- beta_pred # save coefficients for predictions

  mu <- exp(X %*% beta_pred)
  fit$theta <- exp(beta_est[length(beta_est)])

  x_names <- append(x_names, 'ln(theta)')
  names(fit$estimate) <- x_names

  fit$predictions <- mu

  fit$formula <- formula
  fit$observed <- y
  fit$residuals <- y - fit$predictions
  fit$LL <- fit$maximum # The log-likelihood of the model
  fit$modelType <- "poisLind"

  return(fit)
}
