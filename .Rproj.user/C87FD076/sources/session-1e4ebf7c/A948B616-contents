---
title: "Alternative Count Models"
author: "Jonathan Wood"
format: docx
editor: 
  markdown: 
    wrap: 72
---

## Estimating flexible Negative Binomial Models in R

## Negative Binomial Distribution

A common formulation of the negative binomial distribution is based on
Bernoulli trials with a complexity parameter. This version of the
negative binomial has a PDF of:
$$f(y|p,r)=\frac{\Gamma(y+r)}{y!\Gamma(r)}(1-p)^y p^r$$ 
This is related
to common formulations using: $$\mu=\frac{r}{p}$$
$$r=\frac{1}{\alpha}=\frac{\mu^2}{\sigma^2-\mu}$$
$$p=\frac{\mu}{\sigma^2}=\frac{r}{r+\mu}$$ Where $\mu=$ the predicted
value for the outcome (or mean value), $\alpha=$ the overdispersion
parameter, and $\sigma^2=$ the variance.

## Poisson-Gamma Distribution

The Poisson-Gamma mixture model is an alternative formulation of the
negative binomial. This mixture allows for multiple versions of the
negative binomial, including NB1, NB2, and NBP formulations. These are
related to the negative binomial distribution described above. The
equivalent of the negative binomial distribution described above is the
NB2. The NB2 PDf is:
$$f_{NB2}(y|\mu,\alpha)=\frac{\Gamma\left(y+\frac{1}{\alpha}\right)}{\Gamma\left(\frac{1}{\alpha}\right)\Gamma(y+1)}\left(\frac{\mu}{\mu+\frac{1}{\alpha}}  \right)^y\left(\frac{\frac{1}{\alpha}}{\frac{1}{\alpha}+\mu} \right)^{\frac{1}{\alpha}}$$

with LL:
$$LL=\sum \left(\ln\left(\Gamma\left(y+\frac{1}{\alpha}\right)\right) - \ln\left(\Gamma\left(\frac{1}{\alpha}\right)\right) - \ln\left(\Gamma(y+1)\right) + y\left(\ln\left(e^{X_1 \beta_1+x\beta}\right) - \ln\left(e^{X_1 \beta_1+x\beta}+\frac{1}{\alpha}\right)\right) + \frac{1}{\alpha}\left(\ln\left(\frac{1}{\alpha}\right) - \ln\left(\frac{1}{\alpha}+e^{X_1 \beta_1+x\beta}\right)\right)\right)$$
Where $X_1$ represents a single variable that is associated with coefficient $\beta_1$, $x$ represents the remaining independent variables associated with coefficients $\beta$, $y$ is the outcome, and $alpha$ is the overdispersion parameter.

Taking derivatives:
$$\frac{d LL}{d\beta_1} = \sum X_1 \left( y - \left( y + \frac{1}{\alpha} \right)\frac{\mu}{\mu+\frac{1}{\alpha}} \right)
$$

$$\frac{\partial LL}{\partial \alpha} = \sum \frac{1}{\alpha^2} \left( -y\ln\left(1 + \frac{\alpha}{\mu}\right) + \ln\left(\frac{1}{\alpha\mu + 1}\right) \right)$$



$$LL=\sum \left(\ln\left(\Gamma\left(y+\frac{1}{\alpha}\right)\right) - \ln\left(\Gamma\left(\frac{1}{\alpha}\right)\right) - \ln\left(\Gamma(y+1)\right) + y\left(\ln\left(e^{X_1 (\beta_1+\gamma)+x\beta}\right) - \ln\left(e^{X_1 (\beta_1+\gamma)+x\beta}+\frac{1}{\alpha}\right)\right) + \frac{1}{\alpha}\left(\ln\left(\frac{1}{\alpha}\right) - \ln\left(\frac{1}{\alpha}+e^{X_1 (\beta_1+\gamma)+x\beta}\right)\right)\right)$$
where $\gamma$ captures the error term for the random parameter.
$$\frac{\partial LL}{\partial \beta_1} = \sum X_1 \left( y - \left( y + \frac{1}{\alpha} \right)\frac{e^{X_1 (\beta_1+\gamma) + x\beta}}{e^{X_1 (\beta_1+\gamma) + x\beta}+\frac{1}{\alpha}} \right)$$
$$\frac{\partial LL}{\partial \gamma} = \sum X_1 \left( y - \left( y + \frac{1}{\alpha} \right)\frac{e^{X_1 (\beta_1+\gamma) + x\beta}}{e^{X_1 (\beta_1+\gamma) + x\beta}+\frac{1}{\alpha}} \right)$$


The mean and variance for the NB2 are: $$\mu=exp(X\cdot\beta)$$
$$Var(\mu)=\mu+\alpha\cdot\mu^2$$ Where $X=$ a matrix of independent
variables and $\beta=$ a vector of coefficients.

The PDF, mean, and variance for the NB1 are:
$$f_{NB1}(y|\mu,\alpha)=\frac{\Gamma\left(y+\frac{\mu}{\alpha}\right)}{\Gamma\left(\frac{\mu}{\alpha}\right)\Gamma(y+1)}\left(\frac{\mu}{\mu+\frac{\mu}{\alpha}}  \right)^y\left(\frac{\frac{\mu}{\alpha}}{\frac{\mu}{\alpha}+\mu} \right)^{\frac{\mu}{\alpha}}$$
$$\mu_{NB1}=exp(X\cdot\beta)$$
$$Var_{NB1}(\mu_{NB1})=\mu_{NB1}+\alpha\cdot\mu_{NB1}$$

The PDF, mean, and variance for the NBP are:
$$f_{NBP}(y|\mu,\alpha, P)=\frac{\Gamma\left(y+\frac{\mu^{2-P}}{\alpha}\right)}{\Gamma\left(\frac{\mu^{2-P}}{\alpha}\right)\Gamma(y+1)}\left(\frac{\mu}{\mu+\frac{\mu^{2-P}}{\alpha}}  \right)^y\left(\frac{\frac{\mu^{2-P}}{\alpha}}{\frac{\mu^{2-P}}{\alpha}+\mu} \right)^{\frac{\mu^{2-P}}{\alpha}}$$
$$\mu_{NBP}=exp(X\cdot\beta)$$
$$Var_{NBP}(\mu_{NBP})=\mu_{NBP}+\alpha\cdot\frac{\mu_{NBP}^2}{\mu_{NBP}^{2-P}}=\mu_{NBP}+\alpha\cdot\mu_{NBP}^P$$

Where $P=$ an additional shape parameter

The NB2 and NB1 formulations are special cases of the NBP where $P=1$
for the NB1 and $P=2$ for the NB2.

The log-likelihood function for the NBP is:
$$LL=\ln \left( \frac{\Gamma \left( y + \frac{\mu^{2-P}}{\alpha} \right)}{\Gamma \left( \frac{\mu^{2-P}}{\alpha} \right) \Gamma (y + 1)} \right) + y \ln \left( \frac{\mu}{\mu + \frac{\mu^{2-P}}{\alpha}} \right) + \frac{\mu^{2-P}}{\alpha} \ln \left( \frac{\frac{\mu^{2-P}}{\alpha}}{\frac{\mu^{2-P}}{\alpha} + \mu} \right)$$
$$LL=ln\left(\Gamma\left(y+\frac{\mu^{2-P}}{\alpha}\right)\right)-ln\left(\Gamma\left(\frac{\mu^{2-P}}{\alpha} \right)\Gamma(y+1)\right)+y\left(ln(\mu)-ln\left(\mu+\frac{\mu^{2-P}}{\alpha}\right)\right)+\frac{\mu^{2-P}}{\alpha}\left(ln\left(\frac{\mu^{2-P}}{\alpha}\right)-ln\left(\mu+\frac{\mu^{2-P}}{\alpha}\right)\right)$$

However, the estimation implementation in R will use $ln(\alpha)$ and $ln(p)$. Thus, we need the derivative and second derivative for these parameters with respect to $ln(\alpha)$ and $ln(p)$. Additionally, $\mu=e^{X\beta}$, and we want the gradient and Hessian for the $\geta$ parameters.
```{r, echo=FALSE}
use_python("C:/ProgramData/chocolatey/bin/python3.12.exe", required = TRUE)
library(reticulate)
conda_create("r-reticulate")
conda_install("r-reticulate", "sympy")
# Use the newly created Conda environment
use_condaenv("r-reticulate", required = TRUE)
py_config()
```



$$LL=y \left(- ln{\left(e^{X \beta} + \frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)} + ln{\left(e^{X \beta} \right)}\right) - \\ \log{\left(\Gamma\left(\frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha}\right) \Gamma\left(y + 1\right) \right)} + ln\left(\Gamma\left(y + \frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)\right) + \\ \frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha}\left(ln{\left(\frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)} - ln{\left(e^{X \beta} + \frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)}\right) $$
Or:
$$LL=ln\left(\Gamma\left(y+\frac{\mu^{2-P}}{\alpha}\right)\right)-\\ln\left(\Gamma\left(\frac{\mu^{2-P}}{\alpha} \right)\Gamma(y+1)\right)+\\ y\left(ln(\mu)-ln\left(\mu+\frac{\mu^{2-P}}{\alpha}\right)\right)+\\\frac{\mu^{2-P}}{\alpha}\left(ln\left(\frac{\mu^{2-P}}{\alpha}\right)-ln\left(\mu+\frac{\mu^{2-P}}{\alpha}\right)\right)$$
if:
$$S_{sum}=y+\frac{\mu^{2-P}}{\alpha}$$
$$M_{sum}=\mu+\frac{\mu^{2-P}}{\alpha}$$
$$R=\frac{\mu^{2-P}}{\alpha}$$
$$\mu=e^{X\beta}$$

$$LL=ln\left(\Gamma\left(S_{sum}\right)\right)-ln\left(\Gamma\left(R\right)\Gamma(y+1)\right)+y\left(ln(\mu)-ln\left(M_{sum}\right)\right)+
R\left(ln\left(R\right)-ln\left(M_{sum}\right)\right)$$

#### For parameter ln(P)

$$\frac{\partial LL}{\partial \ln(P)} = \frac{P \cdot R \left( \alpha y - \alpha \mu + \left( \alpha \mu + \mu^{2-P} \right) \left( \ln(M_{sum}) - \ln(R) - \psi(S_{sum}) + \psi(R) \right) \right) \ln(\mu)}{\alpha M_{sum}}$$

And:
$$\frac{\partial^2 LL}{\partial \ln(P)^2} = \frac{P \ln(\mu) \left( - P \left( \mu^{2-P} M_{sum} \left( - \mu + y - M_{sum} \left( \ln(R) - \ln(M_{sum}) - \psi(R) + \psi(S_{sum}) \right) \right) + \\
\mu^{4-2P} \left( \mu - y + M_{sum} \left( \ln(R) - \ln(M_{sum}) - \psi(R) + \psi(S_{sum}) \right) \right) +\\
\mu^{2-P} M_{sum} \left( \mu^{2-P} \left( - \ln(R) + \ln(M_{sum}) + \psi(R) - \psi(S_{sum}) \right) + \mu^{2-P} - M_{sum} - \\
R M_{sum} \left( - \psi'(R) + \psi'(S_{sum}) \right) \right) \right) - \mu^{2-P} M_{sum} \left( \mu - y + M_{sum} \left( \ln(R) - \ln(M_{sum}) - \psi(R) + \psi(S_{sum}) \right) \right) \right)}{\alpha (\alpha M_{sum})^2}$$

Where $psi()$ is the digamma function and $\psi'()$ is the trigamma function.

#### When alpha is a constant
$$\frac{\partial LL}{\partial \ln(\alpha)} = \frac{R \left( \alpha y - \alpha \mu + (\alpha \mu + \mu^{2-P}) (\ln(M_{sum}) - \ln(R) - \psi(S_{sum}) + \psi(R)) \right)}{\alpha M_{sum}}$$

Note that:
$$\frac{\partial LL}{\partial \ln(P)}=P\cdot ln(\mu)\frac{\partial LL}{\partial \ln(\alpha)}$$

$$\frac{\partial^2 LL}{\partial \ln(\alpha)^2} = \frac{-\alpha^2 \mu^{3-P} \left( -\alpha \mu + \alpha y + M_{sum} (-\ln(R) + \ln(M_{sum}) + \psi(R) - \psi(S_{sum})) \right) - \\
2\alpha \mu^{2-P} M_{sum} \left( -\alpha \mu + \alpha y + M_{sum} (-\ln(R) + \ln(M_{sum}) + \psi(R) - \psi(S_{sum})) \right) + \\
\alpha \mu^{2-P} M_{sum} \left( -\alpha \mu + \alpha y - M_{sum} (\ln(R) - \ln(M_{sum}) - \psi(R) + \psi(S_{sum})) \right) - \\
\mu^{2-P} M_{sum} \left( \alpha^2 (-\mu (-\ln(R) + \ln(M_{sum}) + \psi(R) - \psi(S_{sum})) + \mu - y) + \\
\alpha \mu^{2-P} - \alpha M_{sum} + \mu^{2-P} M_{sum} \psi'(R) - \mu^{2-P} M_{sum} \psi'(S_{sum}) \right)}{\alpha^2 M_{sum}^2}$$

Note that:

```{python}
import sympy as sp
from sympy import latex

# Define the symbols
alpha, mu, P, y = sp.symbols('alpha mu P y')
M_sum, R = sp.symbols('M_sum R')
psi_R, psi_S_sum, psi_prime_R, psi_prime_S_sum = sp.symbols('psi_R psi_S_sum psi_prime_R psi_prime_S_sum')

# Define the terms
term1 = - P * (mu**(2-P) * M_sum * (- mu + y - M_sum * (sp.log(R) - sp.log(M_sum) - psi_R + psi_S_sum)))
term2 = mu**(4-2*P) * (mu - y + M_sum * (sp.log(R) - sp.log(M_sum) - psi_R + psi_S_sum))
term3 = mu**(2-P) * M_sum * (mu**(2-P) * (- sp.log(R) + sp.log(M_sum) + psi_R - psi_S_sum) + mu**(2-P) - M_sum - R * M_sum * (- psi_prime_R + psi_prime_S_sum))

# Combine the terms inside the parenthesis
inner_terms = term1 + term2 + term3

# Define the numerator and denominator
numerator = P * sp.log(mu) * (inner_terms - mu**(2-P) * M_sum * (mu - y + M_sum * (sp.log(R) - sp.log(M_sum) - psi_R + psi_S_sum)))
denominator = alpha * (alpha * M_sum)**2

# Define the expression
expression = numerator / denominator

# Simplify the expression
simplified_expression_logP = sp.simplify(expression)

# Display the simplified expression
sp.pprint(simplified_expression_logP, use_unicode=True)

print(latex(simplified_expression_logP))


# ln(alpha)

# Define the terms
term1 = -alpha**2 * mu**(3-P) * (-alpha * mu + alpha * y + M_sum * (-sp.log(R) + sp.log(M_sum) + psi_R - psi_S_sum))
term2 = -2 * alpha * mu**(2-P) * M_sum * (-alpha * mu + alpha * y + M_sum * (-sp.log(R) + sp.log(M_sum) + psi_R - psi_S_sum))
term3 = alpha * mu**(2-P) * M_sum * (-alpha * mu + alpha * y - M_sum * (sp.log(R) - sp.log(M_sum) - psi_R + psi_S_sum))
term4 = -mu**(2-P) * M_sum * (alpha**2 * (-mu * (-sp.log(R) + sp.log(M_sum) + psi_R - psi_S_sum) + mu - y) + alpha * mu**(2-P) - alpha * M_sum + mu**(2-P) * M_sum * psi_prime_R - mu**(2-P) * M_sum * psi_prime_S_sum)

# Combine the terms
numerator = term1 + term2 + term3 + term4
denominator = alpha**2 * M_sum**2

# Define the expression
expression = numerator / denominator

# Simplify the expression
simplified_expression = sp.simplify(expression)

# Display the simplified expression
sp.pprint(simplified_expression, use_unicode=True)
print(latex(simplified_expression))


ratio = simplified_expression-simplified_expression_logP
ratio = sp.simplify(ratio)
# Display the simplified expression
sp.pprint(ratio, use_unicode=True)
print(latex(ratio))

```



#### When alpha is a function of predictors
$$\frac{\partial LL}{\partial \beta_{\alpha}} = X_{\alpha}^T \left( \frac{R \left( \alpha y - \alpha \mu + (\alpha \mu + \mu^{2-P}) (\ln(M_{sum}) - \ln(R) - \psi(S_{sum}) + \psi(R)) \right)}{\alpha M_{sum}} \right)$$

$$\frac{\partial^2 LL}{\partial \beta_{\alpha}^2} = X_{\alpha}^T \left( \frac{\partial^2 LL}{\partial \ln(\alpha)^2} \right) X_{\alpha}$$

#### For regression coefficients
$$\frac{\partial LL}{\partial \beta} = \frac{X R \left( \alpha y (P - 1) - \alpha \mu + (P - 2) \left( \alpha M_{sum} (\ln(M_{sum}) - \ln(R) - \psi(S_{sum}) + \psi(R)) + \mu^{2 - P} \right) \right)}{\alpha M_{sum}}$$

And

$$\frac{\partial^2 LL}{\partial \beta^2} = \frac{X^2 \left( - (P - 2) M_{sum} \left( y (P - 1) - \mu + (P - 2) M_{sum} \left( \ln(M_{sum}) - \ln(R) - \psi(S_{sum}) + \psi(R) \right) - (P - 2) M_{sum} + \mu^{2 - P} \right) \mu^{2 - P} \right)}{M_{sum}^2}$$




Note that:
$$\frac{\delta LL}{\delta ln(\alpha)}=\frac{\delta LL}{\delta ln(P)}\cdot \frac{1}{P\cdot ln(\mu)}$$


$$\frac{\delta LL}{\delta \beta}=\frac{X \left(\alpha y \left(P - 1\right) - \alpha e^{X \beta} + \left(P - 2\right) \left(\alpha e^{X \beta} + \left(e^{X \beta}\right)^{2 - P}\right) \left(\log{\left(\frac{\alpha e^{X \beta} + \left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)} - \log{\left(\frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)} - \psi\left(\frac{\alpha y + \left(e^{X \beta}\right)^{2 - P}}{\alpha} \right) + \psi\left(\frac{\left(e^{X \beta}\right)^{2 - P}}{\alpha} \right)\right) - \left(P - 2\right) \left(\alpha e^{X \beta} + \left(e^{X \beta}\right)^{2 - P}\right) + \left(P - 2\right) \left(e^{X \beta}\right)^{2 - P}\right) \left(e^{X \beta}\right)^{2 - P}}{\alpha \left(\alpha e^{X \beta} + \left(e^{X \beta}\right)^{2 - P}\right)}$$

Or:

$$\frac{\delta LL}{\delta \beta}=\frac{XR \left(\alpha y \left(P - 1\right) - \alpha \mu + \left(P - 2\right) \left(\alpha M_{sum}\right) \left(\log{\left(M_{sum} \right)} - \\ \log{\left(R\right)} - \psi\left(M_{sum}\right) + \psi\left(R \right)\right) - \left(P - 2\right) \left(\alpha M_{sum}\right) + \left(P - 2\right) \left(\mu\right)^{2 - P}\right) }{\alpha M_{sum}}$$
Which simplifies to:
$$\frac{\delta LL}{\delta \beta} = \frac{XR \left[ \alpha y (P - 1) - \alpha \mu + (P - 2) \left( \alpha M_{sum} \left( \log(M_{sum}) - \log(R) - \psi(M_{sum}) + \psi(R) \right) + \mu^{2 - P} \right) \right]}{\alpha M_{sum}}$$

```{r}
library(reticulate)
py_install("sympy")
py_install("numpy")
```

```{python}
import sympy as sp
from sympy import latex
import numpy

# Define the variables 
y, P, alpha, X, beta, mu, S, M, R = sp.symbols('y P alpha X beta mu S M R')
#y, P, alpha, X, beta = sp.symbols('y P alpha, X, beta')

# Define the additional variables for the linear predictor
#mu = sp.exp(X*beta)  # mu = exp(X * beta)
mu_pow = mu**(2 - P)
beta_val = mu_pow / alpha

# Define the log-likelihood function LL
LL = (
    sp.loggamma(y + beta_val)
    - sp.log(sp.gamma(beta_val)*sp.gamma(y+1))
    + y * (sp.log(mu) - sp.log(mu + beta_val))
    + beta_val * (sp.log(beta_val) - sp.log(mu + beta_val))
)

# Convert LL to a function of lnP and lnalpha
#LL = LL.subs({P: sp.exp(lnP), alpha: sp.exp(lnalpha)})

# Compute the first derivatives
dLL_dlnP = sp.simplify(sp.diff(LL, P))
dLL_dlnalpha = sp.simplify(sp.diff(LL, alpha))
dLL_dbeta = sp.simplify(sp.diff(LL, beta))

# Compute the second derivatives
d2LL_dlnP2 = sp.simplify(P*(dLL_dlnP+P*sp.diff(dLL_dlnP, P)))
d2LL_dlnalpha2 = sp.simplify(alpha*(dLL_dlnalpha+alpha*sp.diff(dLL_dlnalpha, alpha)))
d2LL_dbeta2 = sp.simplify(sp.diff(dLL_dbeta, beta))

# More
dLL_dlnP = sp.simplify(P*sp.diff(LL, P))
dLL_dlnalpha = sp.simplify(alpha*sp.diff(LL, alpha))

print(latex(dLL_dlnP))
print(latex(dLL_dlnalpha))

sp.pprint(sp.simplify(P*sp.diff(dLL_dlnalpha, P)))


dLL_dbeta = dLL_dbeta.subs(mu**(2-P)/alpha, R)
dLL_dbeta = dLL_dbeta.subs(mu+mu**(2-P)/alpha, M)
dLL_dbeta = dLL_dbeta.subs(y+mu**(2-P)/alpha, S)
dLL_dbeta = sp.simplify(d2LL_dP2)

print(latex(dLL_dbeta))

d2LL_dbeta2 = d2LL_dbeta2.subs(mu**(2-P)/alpha, R)
d2LL_dbeta2 = d2LL_dbeta2.subs(mu+mu**(2-P)/alpha, M)
d2LL_dbeta2 = d2LL_dbeta2.subs(y+mu**(2-P)/alpha, S)
d2LL_dbeta2 = sp.simplify(d2LL_dbeta2)

print(latex(d2LL_dbeta2))

# Function to generate LaTeX code
def generate_latex(expr):
    return sp.latex(expr)
  
sp.simplify(P*dLL_dlnP/(alpha*dLL_dlnalpha))

d2LL_dP2 = sp.diff(sp.diff(LL, P),P)
d2LL_dP2 = d2LL_dP2.subs(mu**(2-P)/alpha, R)
d2LL_dP2 = d2LL_dP2.subs(mu+mu**(2-P)/alpha, M)
d2LL_dP2 = d2LL_dP2.subs(y+mu**(2-P)/alpha, S)
d2LL_dP2 = sp.simplify(d2LL_dP2)

print(latex(d2LL_dP2))

d2LL_da2 = sp.diff(sp.diff(LL, alpha),alpha)
d2LL_da2 = d2LL_dP2.subs(mu**(2-P)/alpha, R)
d2LL_da2 = d2LL_dP2.subs(mu+mu**(2-P)/alpha, M)
d2LL_da2 = d2LL_dP2.subs(y+mu**(2-P)/alpha, S)
d2LL_da2 = sp.simplify(d2LL_dP2)

print(latex(d2LL_da2))

print(latex(sp.simplify(d2LL_da2/d2LL_dP2)))




# Generate LaTeX code
LL_latex = generate_latex(LL)
dLL_dlnP_latex = generate_latex(sp.simplify(P*dLL_dlnP))
dLL_dlnalpha_latex = generate_latex(sp.simplify(alpha*dLL_dlnalpha))
dLL_dbeta_latex = generate_latex(dLL_dbeta)

d2LL_dlnP2_latex = generate_latex(d2LL_dlnP2)
d2LL_dlnalpha2_latex = generate_latex(d2LL_dlnalpha2)
d2LL_dbeta2_latex = generate_latex(d2LL_dbeta2)

print("Log-Likelihood (LL):")
print(LL_latex)
print("\nFirst Derivative with respect to lnP:")
print(dLL_dlnP_latex)
print("\nFirst Derivative with respect to lnalpha:")
print(dLL_dlnalpha_latex)
print("\nFirst Derivative with respect to beta:")
print(dLL_dbeta_latex)
print("\nSecond Derivative with respect to (lnP)^2:")
print(d2LL_dlnP2_latex)
print("\nSecond Derivative with respect to (lnalpha)^2:")
print(d2LL_dlnalpha2_latex)
print("\nHessian with respect to beta:")
print(d2LL_dbeta2_latex)

```
$$\frac{\partial^2 LL}{\partial P^2} = \frac{\mu^{2-P} (\ln(\mu))^2 (y - \mu)}{\alpha} + \frac{\partial}{\partial P} \left[ \frac{\mu^{2-P}}{\alpha} \left( \mu^{2-P} \ln(\mu) \left( \frac{-\frac{\mu^{2-P} \ln(\mu)}{\alpha}}{M_{sum}} - \ln(\mu) - \psi'(S_{sum}) \cdot -\frac{\mu^{2-P} \ln(\mu)}{\alpha} + \psi'(R) \cdot -\frac{\mu^{2-P} \ln(\mu)}{\alpha} \right) \right) \right]$$



$$P\frac{\delta^2LL}{\delta P^2}=P\frac{\left(R \alpha^{2} \left(R + \mu\right)^{2} \left(\log{\left(R \right)} - \log{\left(R + \mu \right)} - \psi\left(R \right) + \psi\left(,R + y \right)\right) + R \alpha^{2} \left(R + \mu\right) \left(2 \mu - y\right) - R \left(R \alpha^{2} \left(R + \mu\right) - \mu^{4 - 2 P}\right) + \mu^{4 - 2 P} y + \mu^{4 - 2 P} \left(R + \mu\right)^{2} \left(- \psi'\left(R \right) + \psi'\left(R + y \right)\right)\right) \log{\left(\mu \right)}^{2}}{\alpha^{2} \left(R + \mu\right)^{2}}$$

$$\frac{\delta^2LL}{\delta\alpha^2}=\frac{\left(R \alpha^{2} \left(R + \mu\right)^{2} \left(\log{\left(R \right)} - \log{\left(R + \mu \right)} - \psi\left(R \right) + \psi\left(R + y \right)\right) + R \alpha^{2} \left(R + \mu\right) \left(2 \mu - y\right) - R \left(R \alpha^{2} \left(R + \mu\right) - \mu^{4 - 2 P}\right) + \mu^{4 - 2 P} y + \mu^{4 - 2 P} \left(R + \mu\right)^{2} \left(- \psi'\left(R \right) + \psi'\left(R + y \right)\right)\right) \log{\left(\mu \right)}^{2}}{\alpha^{2} \left(R + \mu\right)^{2}}$$

$$\frac{\delta^2 LL}{\delta\beta^2}=\frac{\left(R \alpha^{2} \left(R + \mu\right)^{2} \left(\log{\left(R \right)} - \log{\left(R + \mu \right)} - \psi\left(R \right) + \psi\left(R + y \right)\right) + R \alpha^{2} \left(R + \mu\right) \left(2 \mu - y\right) - R \left(R \alpha^{2} \left(R + \mu\right) - \mu^{4 - 2 P}\right) + \mu^{4 - 2 P} y + \mu^{4 - 2 P} \left(R + \mu\right)^{2} \left(- \psi'\left(R \right) + \psi'\left(R + y \right)\right)\right)\log{\left(\mu \right)}^{2}}{\alpha^{2} \left(R + \mu\right)^{2}}$$





$$\frac{\delta^2 LL}{\delta ln(P)^2}=\frac{P \log(\mu) \left( - P \left( \mu^{2 - P} M_{sum} \left( - \mu + y - M_{sum} \left( \log (R) - \log (M_{sum}) - \psi (R) + \\
\psi (S_{sum}) \right) \right) + \mu^{4 - 2 P} \left( \mu - y + M_{sum} \left( \log (R) - \log (M_{sum}) - \psi (R) + \psi (S_{sum}) \right) \right) + \\ \mu^{2 - P} M_{sum} \left( \mu^{2 - P} \left( - \log (R) + \log (M_{sum}) + \psi (R) - \psi (S_{sum}) \right) + \mu^{2 - P} - M_{sum} - \\
R M_{sum} \left( - \psi' (R) + \psi' (S_{sum}) \right) \right) \right) - \mu^{2 - P} M_{sum} \left( \mu - y + M_{sum} \left( \log (R) - \log (M_{sum}) - \\
\psi (R) + \psi (S_{sum}) \right) \right) \right)}{\alpha (\alpha M_{sum})^2}$$


$$\frac{\delta^2LL}{\delta ln(\alpha)^2}=\frac{- \alpha^{2} \mu^{3 - P} \left( - \alpha \mu + \alpha y + M_{sum} \left( - \log \left( R \right) + \log \left( M_{sum} \right) + \psi \left( R \right) - \psi \left( S_{sum} \right) \right) \right) - \\
2 \alpha \mu^{2 - P} M_{sum} \left( - \alpha \mu + \alpha y + M_{sum} \left( - \log \left( R \right) + \log \left( M_{sum} \right) + \psi \left( R \right) - \psi \left( S_{sum} \right) \right) \right) + \\
\alpha \mu^{2 - P} M_{sum} \left( - \alpha \mu + \alpha y - M_{sum} \left( \log \left( R \right) - \log \left( M_{sum} \right) - \psi \left( R \right) + \psi \left( S_{sum} \right) \right) \right) - \\
\mu^{2 - P} M_{sum} \left( \alpha^{2} \left( - \mu \left( - \log \left( R \right) + \log \left( M_{sum} \right) + \psi \left( R \right) - \psi \left( S_{sum} \right) \right) + \mu - y \right) + \\
\alpha \mu^{2 - P} - \alpha M_{sum} + \mu^{2 - P} M_{sum} \psi' \left( R \right) - \mu^{2 - P} M_{sum} \psi' \left( S_{sum} \right) \right)}{\alpha^{2} M_{sum}^{2}}$$

$$\frac{\delta^2 LL}{\delta\beta^2}=\frac{X^{2} \left( - (P - 2) M_{sum} \left( y (P - 1) - \mu + (P - 2) M_{sum} \left( \log(M_{sum}) - \log(R) - \psi(S_{sum}) + \psi(R) \right) - (P - 2) M_{sum} + \mu^{2 - P} \right) \mu^{2 - P} \right)}{M_{sum}^{2}}$$








$$\frac{\partial LL}{\partial \ln (P)} = P \left( \frac{\partial LL}{\partial P} \right)$$

And

$$\frac{\partial LL}{\partial \ln (P)}=P\left[\frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - Pa}}{\alpha} \right) \right) + y \left( - \frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) + \\ \frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 - P}}{\alpha} \right) \right) + \frac{\mu^{2 - P}}{\alpha} \left( -\frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right)\right] $$

Where $\psi()$ is the digamma function.

Similarly:

$$\frac{\partial LL}{\partial \ln(\alpha)} = \alpha \left( \frac{\partial LL}{\partial\alpha} \right)$$

$$\frac{\partial LL}{\partial \alpha} = \frac{\mu^{2 - P}}{\alpha^2} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) + y \left( \frac{\mu^{2 - P}}{\alpha^2 (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 -P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 -P}}{\alpha} \right) \right)$$
For the Hessian:

$$\frac{\partial^2 LL}{\partial (\ln P)^2} = P \left( \frac{\partial}{\partial P} \left( P \frac{\partial LL}{\partial P} \right) \right)=P \left( \frac{\partial LL}{\partial P} \right) + P^2 \left( \frac{\partial^2 LL}{\partial P^2} \right)$$

















$$\frac{\partial LL}{\partial P} = \frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) + y \left( - \frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) + \frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 - P}}{\alpha} \right) \right) + \frac{\mu^{2 - P}}{\alpha} \left( -\frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right)$$

We need to find:

$$\frac{\partial}{\partial P} \left( P \frac{\partial LL}{\partial P} \right) = \frac{\partial}{\partial P} \left( P \left[ \frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) + y \left( - \frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) + \frac{\mu^{2 - P} \ln \mu}{\alpha} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 - P}}{\alpha} \right) \right) + \frac{\mu^{2 - P}}{\alpha} \left( -\frac{\mu^{2 - P} \ln \mu}{\alpha (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) \right] \right)$$

Using the product rule:
$$
= \frac{\partial P}{\partial P} \cdot \frac{\partial LL}{\partial P} + P \cdot \frac{\partial}{\partial P} \left( \frac{\partial LL}{\partial P} \right)$$

$$= \frac{\partial LL}{\partial P} + P \cdot \frac{\partial}{\partial P} \left( \frac{\partial LL}{\partial P} \right)
$$

$$
\frac{\partial^2 LL}{\partial (\ln \alpha)^2} = \alpha \left( \frac{\partial}{\partial \alpha} \left( \alpha \frac{\partial LL}{\partial \alpha} \right) \right)$$

Where:

$$\frac{\partial LL}{\partial \alpha} = \frac{\mu^{2 - P}}{\alpha^2} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) + y \left( \frac{\mu^{2 - P}}{\alpha^2 (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 - P}}{\alpha} \right) \right)$$

We need to find:

$$\frac{\partial}{\partial \alpha} \left( \alpha \frac{\partial LL}{\partial \alpha} \right) = \frac{\partial}{\partial \alpha} \left( \alpha \left[ \frac{\mu^{2 - P}}{\alpha^2} \left( \psi \left( y + \frac{\mu^{2 - P}}{\alpha} \right) - \psi \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) \right) + y \left( \frac{\mu^{2 - P}}{\alpha^2 (\mu + \frac{\mu^{2 - P}}{\alpha})} \right) - \frac{\mu^{2 - P}}{\alpha^2} \left( \ln \left( \frac{\mu^{2 - P}}{\alpha} \right) - \ln \left( \mu + \frac{\mu^{2 - P}}{\alpha} \right) \right) \right] \right)$$

Using the product rule:
$$= \frac{\partial \alpha}{\partial \alpha} \cdot \frac{\partial LL}{\partial \alpha} + \alpha \cdot \frac{\partial}{\partial \alpha} \left( \frac{\partial LL}{\partial \alpha} \right)$$

$$= \frac{\partial LL}{\partial \alpha} + \alpha \cdot \frac{\partial}{\partial \alpha} \left( \frac{\partial LL}{\partial \alpha} \right)$$




## Overdispersion as a function - the generalized negative binomial

Additionally, each of these could have the overdispersion parameter
$\alpha$ parameterized as a function of predictor variables. In this
case, the following formulation is used:
$$ln(\alpha)=\beta_0+\sum_{i=1}^n \beta_i x_i$$

This results in: $$\alpha=exp(\beta_0)\prod_{i=1}^n exp(x_i)^{\beta_1}$$

These formulas are utilized to develop a function that can estimate the
NB1, NB2, and NBP with or without the overdispersion parameter being a
function of independent variables.

```{r}
#install.packages("maxLik")
library(nlme)
library(maxLik)
library(MASS)

nbg.reg <- function(formula, data, form = 'nb2', ln.alpha.formula = NULL, method = 'BHHH', max.iters=200) { 
  # This function estimates alternative forms of the negative binomial regression
  # The input parameter `form` can have the values "nb1" (NB1 distribution), "nb2" (standard negative binomial form), or "nbp" (NB-P Distribution)
  # The formula follows standard R syntax
  # The data input is a dataframe that the model should be estimated on
  # ln.alpha.formula allows the natural log of the overdispersion parameter to be defined as a function of variables. If this option is used, the formula should be in the form: `~ var1 + var2`. The result is the natural log of the overdispersion parameter is: `ln(alpha) = intercept + beta1 * var1 + beta2 * var2` Accounting for the properties of log transforms makes this a flexible model specification where the overdispersion is a function of the independent variables.
  # `method` can take any estimation method allowed in the maxLik library (e.g., 'NM', 'NR', 'BFGS', 'SANN', 'BHHH", etc.)
  # max.iters is the maximum number of iterations that the optimization method is allowed to run. This is passed to the maxLik function as the iterlim parameter of the `maxLik` function.

  mod_df <- stats::model.frame(formula, data)
  X <- stats::model.matrix(formula, data)
  y <- as.numeric(model.response(mod_df))
  x_names <- Names(formula, data)
  
  # Use the Poisson as starting values
  p_model <- MASS::glm.nb(formula, data = data)
  start <- unlist(p_model$coefficients)
  a <- log(1/p_model$theta)
  
  if (is.null(ln.alpha.formula)){
    if (!is.null(a)) {
      comb_start <- append(start, a)
    }
  }else{
    alpha_X <- stats::model.matrix(ln.alpha.formula, data)
    alpha_names <- Names(ln.alpha.formula, data)
    a_coefs <- rep(0, length(alpha_names))
    a_coefs[1] = a
    if (!is.null(a_coefs)) {
      comb_start <- c(start, a_coefs)
    }
  }

  if (!is.null(comb_start) & form=='nbp') {
    full_start <- append(comb_start, 1.5)
  } else{
    full_start <- comb_start
  }
  
  modparams <- as.numeric(length(start)) # save the number of model coefficients, not including alpha or P
  
  nbp_prob <- function(observed, predicted, log_alpha, p) {
      alpha <- exp(log_alpha)
      mu <- predicted 
      y <- observed
      if (form=='nb1'){
        return(dnbinom(y, size = alpha, mu = mu))
      } else if (form=='nb2'){
        return(dnbinom(y, size = mu/alpha, mu = mu))
      } else{
        return(dnbinom(y, size = (mu^(2-p))/alpha, mu = mu))
      }
  }
  
  reg.run <- function(beta, y, X, est_method){

    params_split <- split(beta,ceiling(seq_along(beta) / modparams))

    coefs <- as.vector(unlist(params_split[1]))
    dist_params <- as.vector(unlist(params_split[2]))
    
    if (form=='nbp'){
      if (is.null(ln.alpha.formula)){
        log_alpha <- dist_params[1]
        p <- dist_params[2]
      }else{
        alpha_coefs <- dist_params[1:(length(dist_params)-1)]
        log_alpha <- alpha_X %*% alpha_coefs
        p <- dist_params[length(dist_params)]
      }
    } else{
        if (is.null(ln.alpha.formula)){
          log_alpha <- dist_params[1]
        }else{
          alpha_coefs <- dist_params
          log_alpha <- alpha_X %*% alpha_coefs
        }
      p <- NULL
    }

    predicted <- exp(X %*% coefs)
    
    probs <- nbp_prob(y, predicted, log_alpha, p)
    
    ll <- sum(log(probs))
    if (est_method == 'bhhh' | est_method == 'BHHH'){
      return(log(probs))
    } else{return(ll)}
  }
  
  fit <- maxLik(reg.run, 
                start = full_start,
                y = y,
                X = X,
                est_method = method,
                method = method,
                iterlim = max.iters)

  beta_est <- fit$estimate
  
  betas <- split(beta_est,ceiling(seq_along(beta_est) / modparams))
  beta_pred <- as.vector(unlist(betas[1]))
  
  if (is.null(ln.alpha.formula)){
    x_names <- append(x_names, 'ln(alpha)')
  }else{
    x_names <- append(x_names, paste('ln(alpha): ', alpha_names))
  }
  
  if (form=='nbp'){
    x_names = append(x_names, 'P')
  }
  
  names(fit$estimate) <- x_names
  fit$beta_pred <- beta_pred # save coefficients for predictions
  fit$formula <- formula
  fit$modelType <- form
  fit$predictions <- exp(X %*% beta_pred)
  fit$observed <- y
  fit$residuals <- y - fit$predictions
  fit$LL <- fit$maximum # The log-likelihood of the model
  
  # Estimate Poisson model for tests and pseudo R^2
  pois_mod <- glm(formula, data, family = poisson(link = "log"))
  base_mod <- glm(y ~ 1, family = poisson(link = "log"))
  
  LLpoisson <- sum(dpois(pois_mod$y, pois_mod$fitted.values, log=TRUE))
  LLbase <- sum(dpois(base_mod$y, base_mod$fitted.values, log=TRUE))
  
  fit$LR <- -2*(LLpoisson - fit$LL) # LR Statistic
  fit$LRdof <- length(x_names) - length(pois_mod$coefficients) # LR Degrees of Freedom
  if (fit$LR>0) {
    fit$LR_pvalue <- pchisq(fit$LR, fit$LRdof, lower.tail=FALSE)  # LR p-Value
  }else{
    fit$LR_pvalue <- 1
  }
  
  # Compute McFadden's Pseudo R^2, based on a Poisson intercept-only model
  fit$PseudoR2 <- 1-fit$LL/LLbase
  
  
  # Print out key model metrics
  LRpval <- ifelse(fit$LR_pvalue<0.0001, "<0.0001", round(fit$LR_pvalue,4))
  print('The Likelihood Ratio (LR) Test for H0: NB is No Better than Poisson')
  print(paste('LR = ', round(fit$LR,4)))
  print(paste('LR degrees of freedom = ', fit$LRdof))
  print(paste('LR p-value = ', LRpval))
  print(paste("Macfadden's Pseudo R^2 = ", round(fit$PseudoR2,4)))
  
  # Note that this has the predictions, residuals, observed outcome, LR test, and pseudo-r^2 stored with the model
  
  return(fit)
}

nbg.predict <- function(model, data){
  # This function takes in an nbg.reg model object and a dataframe
  # The function returns a dataframe with predictions, observed outcome, and residuals for the data that was input

  mod_df <- model.frame(model$formula, data)
  X <- model.matrix(model$formula, data)
  y <- as.numeric(model.response(mod_df))
  
  beta_pred <- model$beta_pred
  
  predictions <- exp(X %*% beta_pred)
  observed <- y
  residuals <- y - predictions

  pred <- data.frame('prediction'=predictions, 'observed'=observed, 'residuals'=residuals)
  
  return(pred)
}
```

## Tests for Overdispersion

There are several tests that can be applied to determine if count
outcomes are overdispersed. Often,m a Likelihood-Ratio ($LR$) test
between the negative binomial and the simpler Poisson regression is used
to determine if the addition of the additional distributional parameters
to the model are are justified. In this case, the Poisson and Negative
Binomial models are both estimated. The Poisson is the special case
where $\alpha=0$. Thus, the $LR$ test is based on $H_0:\alpha=0$ with
$H_1:\alpha>0$. The test is computed as:
$$LR=-2\left(LL_{Poisson}-LL_{Negative Binomial} \right)$$ Where: $LL=$
The likelihood Ratio, which follows a $\chi^2$ distribution,
$LL_{Poisson}=$ The log-likelihood of the Poisson model, and
$LL_{Negative Binomial}=$ The log-likelihood of the negative binomial
model. This test has the degree of freedom
$df=n_{Negative Binomial}-n_{Poisson}$ where: $n_{Negative Binomial}=$
The number of parameters estimated in the negative binomial model, and
$n_{Poisson}=$ The number of parameters estimated in the Poisson model.

This test is implemented in the modeling function. Additionally, the
McFadden Pseudo-r-square ($\rho^2$) is included, based on an
intercept-only Poisson model. This is computed as:
$$\rho^2=1-\frac{LL_{Negative Binomial}}{LL_{Intercept-Only \ Poisson}}$$

## Example

For this example, use a dataset to show each model.

```{r}
load("C:/Users/jwood2/Box/RPNB/WA/washington_dataset.Rdata")

washington_roads$AADTover10k <- ifelse(washington_roads$AADT>10000,1,0) # create a dummy variable

head(washington_roads)
```

Start with NBP, with and without overdispersion as a function

```{r}
nbp.base <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k,
                        data=washington_roads,
                        form = 'nbp',
                        ln.alpha.formula = NULL,
                        method = 'NM',
                        max.iters=3000)

summary(nbp.base)
```

Let's also look at the CURE plots for each model.

```{r}
library(cureplots)

res <- nbp.base$residuals
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

Next, try the overdispersion as a function.

```{r}
nbp.overdispersion <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k, 
                                  data=washington_roads, 
                                  form = 'nbp', 
                                  method = 'NM', 
                                  max.iters=3000,
                                  ln.alpha.formula = ~ 1+lnlength)

summary(nbp.overdispersion)
```

```{r}
res <- nbp.overdispersion$residuals
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

Now, run as NB1 models

```{r}
nb1.base <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k,
                        data=washington_roads,
                        form = 'nb1',
                        ln.alpha.formula = NULL,
                        method = 'NM',
                        max.iters=3000)
summary(nb1.base)
```

```{r}
res <- nb1.base$residuals
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

```{r}
nb1.overdispersion <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k, 
                                  data=washington_roads, 
                                  form = 'nb1', 
                                  method = 'NM', 
                                  max.iters=3000,
                                  ln.alpha.formula = ~ 1+lnlength)


summary(nb1.overdispersion)
```

```{r}
res <- nb1.overdispersion$residuals
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

Next, run as NB2 models

```{r}
nb2.base <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k,
                        data=washington_roads,
                        form = 'nb2',
                        ln.alpha.formula = NULL,
                        method = 'NM',
                        max.iters=3000)

summary(nb2.base)
```

```{r}
res <- nb2.base$residuals
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

```{r}
nb2.overdispersion <- nbg.reg(Total_crashes ~ lnaadt + lnlength + speed50 + ShouldWidth04 + AADTover10k, 
                                  data=washington_roads, 
                                  form = 'nb2', 
                                  method = 'BHHH', 
                                  max.iters=3000,
                                  ln.alpha.formula = ~ 1+lnaadt)

summary(nb2.overdispersion)
```

```{r}
res <- exp(nb2.overdispersion$residuals)
AADT <- washington_roads$AADT
cure_df <- calculate_cure_dataframe(AADT, res)
cure_plot(cure_df)
```

## Poisson-Lindley

### Lindley Distribution

There are numerous forms of the Lindley distribution. These include on
parameter to 7 or more parameter formulations of the Lindley
distribution. There are also version of the Lindley distribution that
are specifically restricted to the range \[0,1\], known as a unit
Lindley distribution.

The original one parameter Lindley distribution has a PDF and CDF as
shown below. $$f(y)=\frac{\theta^2}{\theta+1}(1+y)e^{-\theta \cdot y}$$
$$F(y)=1-\frac{\theta+1+\theta\cdot y}{\theta+1}e^{-\theta \cdot y}$$

Where $y>0$ and $\theta>0$.

This one parameter Lindley distribution is a mixture of an exponential
distribution and a gamma distribution, as shown below.
$$f(y)=\frac{\theta^2}{\theta+1}(1+y)e^{-\theta \cdot y}=\left(\frac{1}{\theta+1}\right)Gamma(2,\theta)+\left(\frac{\theta}{\theta+1}\right)Exp(\theta)$$

This mixture of the gamma and exponential distributions shows that the
weights given to the gamma and exponential distributions are based on
the distribution parameter $\theta$. Additionally, the gamma and
exponential distributions used are based on the parameter $\theta$.

The Lindley distribution has also been described as a mixture of two
gamma distributions. This formulation is:
$$f(y)=\frac{\theta^2}{\theta+1}(1+y)e^{-\theta \cdot y}=\left(\frac{1}{\theta+1}\right)Gamma(2,\theta)+\left(\frac{\theta}{\theta+1}\right)Gamma(1,\theta)$$

It should be noted that the exponential distribution is a special case
of the Gamma distribution when the shape parameter is equal to 1. Thus,
these formulations are equivalent.

### Poisson-Lindley Compound Distribution

An alternative to the Poisson-Gamma distribution for overdispersed data
is the Poisson-Lindley. This distribution derivation is available from the authors on request. The PMF for the Poisson-Lindley is:
$$f(y|\theta,\lambda)=\frac{\theta^2\lambda^y(\theta+\lambda+y+1)}{(\theta+1)(\theta+\lambda)^{y+2}}$$

With: 
$$y\geq0$$ 
$$\theta>0$$
$$\lambda>0$$
The mean and variance of the distribution are:
$$\mu=\lambda\frac{\theta+2}{\theta(\theta+1)}$$
$$Var(\lambda)=\mu+\mu^2\left(1-\frac{2}{(\theta+2)^2} \right)$$

## Poisson-Lindley Regression

The Poisson-Lindley regression can be implemented using the code below.

```{r Poisson Lindley Regression}
#install.packages("maxLik")
library(nlme)
library(maxLik)
library(MASS)

pl.reg <- function(formula, data, method = 'BHHH', max.iters=200) { 
  # The formula follows standard R syntax
  # The data input is a dataframe that the model should be estimated on
  # `method` can take any estimation method allowed in the maxLik library (e.g., 'NM', 'NR', 'BFGS', 'SANN', 'BHHH", etc.)
  # max.iters is the maximum number of iterations that the optimization method is allowed to run. This is passed to the maxLik function as the iterlim parameter of the `maxLik` function.

  mod_df <- stats::model.frame(formula, data)
  X <- stats::model.matrix(formula, data)
  y <- as.numeric(model.response(mod_df))
  x_names <- Names(formula, data)
  
  # Use the Poisson as starting values
  p_model <- MASS::glm.nb(formula, data = data)
  start <- unlist(p_model$coefficients)
  theta <- (1.6661*p_model$theta)^(1/2.142) # This is the theta that gives equivalent variance to the NB2 alpha
  
  start[1] <- start[1]-log((theta+2)/(theta*(theta+1))) # adjust intercept for theta
  
  full_start <- append(start, log(theta))

  modparams <- as.numeric(length(full_start)) 
  
  pl_prob <- function(y, mu, log_theta){
    theta <- exp(log_theta)
    p <- (theta^2*mu^y*(theta+mu+y+1))/((theta+1)*(mu+theta)^(y+2))
  return(p)
  }
  
  reg.run <- function(beta, y, X, est_method){
    pars <- length(beta)-1

    coefs <- as.vector(unlist(beta[1:pars]))
    log_theta <- unlist(beta[length(beta)])

    predicted <- exp(X %*% coefs) # not including Lindley adjustment
    
    probs <- pl_prob(y, predicted, log_theta)
    
    ll <- sum(log(probs))
    if (est_method == 'bhhh' | est_method == 'BHHH'){
      return(log(probs))
    } else{return(ll)}
  }
  
  fit <- maxLik(reg.run, 
                start = full_start,
                y = y,
                X = X,
                est_method = method,
                method = method,
                iterlim = max.iters)

  beta_est <- fit$estimate
  npars <- length(beta_est)-1

  beta_pred <- as.vector(unlist(beta_est[1:npars]))
  fit$beta_pred <- beta_pred # save coefficients for predictions
  
  mu <- exp(X %*% beta_pred)
  fit$theta <- exp(beta_est[length(beta_est)])
  
  x_names <- append(x_names, 'ln(theta)')
  names(fit$estimate) <- x_names

  fit$predictions <- mu*(fit$theta+2)/(fit$theta*(fit$theta+1))

  fit$formula <- formula
  fit$observed <- y
  fit$residuals <- y - fit$predictions
  fit$LL <- fit$maximum # The log-likelihood of the model
  fit$model.type <- "Poisson-Lindley"
  
  # Estimate Poisson model for tests and pseudo R^2
  pois_mod <- glm(formula, data, family = poisson(link = "log"))
  base_mod <- glm(y ~ 1, family = poisson(link = "log"))
  
  LLpoisson <- sum(dpois(pois_mod$y, pois_mod$fitted.values, log=TRUE))
  LLbase <- sum(dpois(base_mod$y, base_mod$fitted.values, log=TRUE))
  
  fit$LR <- -2*(LLpoisson - fit$LL) # LR Statistic
  fit$LRdof <- length(x_names) - length(pois_mod$coefficients) # LR Degrees of Freedom
  if (fit$LR>0) {
    fit$LR_pvalue <- pchisq(fit$LR, fit$LRdof, lower.tail=FALSE)  # LR p-Value
  }else{
    fit$LR_pvalue <- 1
  }
  
  # Compute McFadden's Pseudo R^2, based on a Poisson intercept-only model
  fit$PseudoR2 <- 1-fit$LL/LLbase
  
  
  # Print out key model metrics
  LRpval <- ifelse(fit$LR_pvalue<0.0001, "<0.0001", round(fit$LR_pvalue,4))
  print('The Likelihood Ratio (LR) Test for H0: Poisson-Lindley is No Better than the Poisson')
  print(paste('LR = ', round(fit$LR,4)))
  print(paste('LR degrees of freedom = ', fit$LRdof))
  print(paste('LR p-value = ', LRpval))
  print(paste("Macfadden's Pseudo R^2 = ", round(fit$PseudoR2,4)))
  
  # Note that this has the predictions, residuals, observed outcome, LR test, and pseudo-r^2 stored with the model
  
  return(fit)
}

pl.predict <- function(model, data){
  # This function takes in an nbg.reg model object and a dataframe
  # The function returns a dataframe with predictions, observed outcome, and residuals for the data that was input

  mod_df <- model.frame(model$formula, data)
  X <- model.matrix(model$formula, data)
  y <- as.numeric(model.response(mod_df))
  
  beta_pred <- model$beta_pred
  theta <- model$theta
  mu <- exp(X %*% beta_pred)
  predictions <- mu*(theta+2)/(theta*(theta+1))
  observed <- y
  residuals <- y - predictions

  pred <- data.frame('prediction'=predictions, 'observed'=observed, 'residuals'=residuals, 'predictors'=X)
  
  return(pred)
}
```

Now, run an example:

```{r}

df <- read.csv('C:/Users/jwood2/Box/RPNB/WA/WA_Roads_with_Crashes.csv')
df$length <- df$endmp-df$begmp
df <- df[df$rururb!="U" & df$aadt>0 & df$length>=0.1 & df$no_lanes==2 & df$spd_limt>50 & df$spd_limt<70,]
df$lnaadt <- log(df$aadt)
df$lnlength <- log(df$length)
df$speed60 <- ifelse(df$spd_limt==60,1,0) # baseline is 55 mph

head(df)

```

Note that the default test for log(theta) is if the coefficient is
different from a value of 0 (different from theta=1). This has no
practical meaning. To determine if the model is better than the Poisson,
or other count models, use the LR test, AIC, BIC, etc.

```{r}
pl.base <- pl.reg(Animal ~ lnaadt  + lnlength + speed60,
                        data=df,
                        method = 'nm',
                        max.iters=3000)

summary(pl.base)
```

```{r}
preds <- pl.predict(pl.base, df)
```

```{r}
library(cureplots)
res <- pl.base$residuals
predictions <- pl.base$predictions
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```

```{r}
cure_plot2 <- function(x, covariate = NULL, n_resamples = 0) {
  library(tibble)

  ## Dummy dfns. to avoid warnings while building the package. Not actually
  ## necessary.
  cumres <- NULL
  upper <- NULL
  lower <- NULL


  ## Messages
  msg <- c(
    df_provided = paste0("CURE data frame was provided. Its first column, ",
                         "{cov_name}, will be used.\n"),
    ignore_covariate =
      paste0("Argument covariate = {covariate} will be ignored.\n"),
    missing_covariate =
      paste0("Argument 'covariate' must be provided along with model\n"),
    model_provided =
      paste0("Covariate {covariate} will be used to produce CURE plot.\n")
    )


  ## Check if a data frame or model was received.
  if (is.data.frame(x)) {
    cov_name <- names(x)[1]

    ## Display message
    message(glue::glue(msg[1]))
    if (!is.null(covariate)) message(glue::glue(msg[2]))

    ## Create copy
    plot_df <- x
    dens_df <- tibble(x=plot_df[[cov_name]], y=0)

    ## Copy covariate values
    plotcov__ <- plot_df[[cov_name]]
    residuals <- plot_df[["residual"]]

    ## Case when a model is provided
  } else {

    ## Covariate messages
    if (is.null(covariate)) {
      stop(glue::glue(msg[3]))
    } else {
      message(glue::glue(msg[4]))
    }

    ## Extract covariate and residuals
    cov_name <- covariate
    plotcov__ <- x[["model"]][[covariate]]
    residuals <- residuals(x, type = "working")

    ## Save plot
    plot_df <- suppressMessages(
      calculate_cure_dataframe(plotcov__, residuals)
    )
  }

  ## Rename first column
  names(plot_df) <- c("plotcov__", names(plot_df)[-1])

  ## Create base for ggplot2 object
  out <-
    plot_df |>
    ggplot2::ggplot() +
    ggplot2::aes(x = plotcov__)

  ## Produce resamples (if required) and add them to ggplot2 object.
  if (n_resamples > 0) {

    ## Resample residuals
    resamples_tbl <-
      resample_residuals(plotcov__, residuals, n_resamples)

    ## Add overlay resamples to plot
    out <-
      out +
      ggplot2::geom_line(
        data = resamples_tbl,
        ggplot2::aes(x = plotcov__, y = cumres, group = sample),
        col = "grey"
      )
  }
  
  # bin width
  bw <- 10*(max(plotcov__)-min(plotcov__))/length(plotcov__)

  out +
    ggplot2::geom_line(
      ggplot2::aes(y = cumres), linewidth = 0.9, color = "#112446") +
    ggplot2::geom_line(
      ggplot2::aes(y = upper), linewidth = 0.75, color = "red") +
    ggplot2::geom_line(
      ggplot2::aes(y = lower), linewidth = 0.75, color = "red") +
    #ggplot2::geom_density(aes(y=-40), height=10) +
    ggplot2::geom_dotplot(stackdir = "center",
               stackratio = 0.5,
               dotsize = 0.2,
               method="dotdensity",
               stackgroups = T,
               binpositions="all", 
               binwidth = bw,
               color='grey') +
    ggplot2::labs(x = cov_name, y = "Cumulative Residuals") +
    ggplot2::theme_light()
  
  
}
```

```{r}
cure_plot2(cure_df)
```
```{r}
res <- pl.base$residuals
AADT <- exp(preds$predictors.lnaadt)
cure_df2 <- calculate_cure_dataframe(AADT, res)
cure_plot2(cure_df2)
```

Compare with NB

```{r}
nb.base <- glm.nb(Animal ~ lnaadt + lnlength + speed60,
                        data=df)

summary(nb.base)
```

```{r}
res <- nb.base$residuals
predictions <- nb.base$fitted.values
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot2(cure_df)
```

Comparing fit statistics:

```{r}
library(kableExtra)
rmse <- function(res){
  return(round(sqrt(mean(res^2)),4))
}

mae <- function(res){
  return(round(mean(abs(res)),4))
}

bias <- function(res, obs){
  return(round(sum(res)/sum(obs)*100,4))
}

nb.rmse <- rmse(nb.base$residuals)
nb.mae <- mae(nb.base$residuals)
nb.bias <- bias(nb.base$residuals, nb.base$y)


pl.rmse <- rmse(pl.base$residuals)
pl.mae <- mae(pl.base$residuals)
pl.bias <- bias(pl.base$residuals, pl.base$observed)

# Creating a matrix with the values
values_matrix <- matrix(c("RMSE", "MAE", "Bias (%)",
                          nb.rmse, nb.mae, nb.bias,
                          pl.rmse, pl.mae, pl.bias),
                        nrow = 3, ncol = 3, byrow = FALSE)

# Converting matrix to a data frame
values_df <- as.data.frame(values_matrix)

# Assigning column names
colnames(values_df) <- c("Fit Metric", "Negative Binomial", "Poisson-Lindley")

kbl(values_df, format='simple', booktabs=TRUE)

```

From the CURE plots and the fit statistics, the Poisson-Lindley is
clearly fitting the data better due to the high number of values for 0
or 1 crashes.

### Now, repeat for the Rollover crashes

Note that the default test for log(theta) is if the coefficient is
different from a value of 0 (different from theta=1). This has no
practical meaning. To determine if the model is better than the Poisson,
or other count models, use the LR test, AIC, BIC, etc.

```{r}
pl.base <- pl.reg(Rollover ~ lnaadt + lnlength + speed60,
                        data=df,
                        method = 'sann',
                        max.iters=3000)

summary(pl.base)
```

```{r}
preds <- pl.predict(pl.base, df)
```

```{r}
res <- pl.base$residuals
predictions <- pl.base$predictions
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```

Compare with NB

```{r}
nb.base <- glm.nb(Rollover ~ lnaadt + lnlength + speed60,
                        data=df)

summary(nb.base)
```

```{r}
res <- nb.base$residuals
predictions <- nb.base$fitted.values
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```

Comparing fit statistics:

```{r}
nb.rmse <- rmse(nb.base$residuals)
nb.mae <- mae(nb.base$residuals)
nb.bias <- bias(nb.base$residuals, nb.base$y)


pl.rmse <- rmse(pl.base$residuals)
pl.mae <- mae(pl.base$residuals)
pl.bias <- bias(pl.base$residuals, pl.base$observed)

# Creating a matrix with the values
values_matrix <- matrix(c("RMSE", "MAE", "Bias (%)",
                          nb.rmse, nb.mae, nb.bias,
                          pl.rmse, pl.mae, pl.bias),
                        nrow = 3, ncol = 3, byrow = FALSE)

# Converting matrix to a data frame
values_df <- as.data.frame(values_matrix)

# Assigning column names
colnames(values_df) <- c("Fit Metric", "Negative Binomial", "Poisson-Lindley")

kbl(values_df, format='simple', booktabs=TRUE)

```

## Poisson-Lindley-Lognormal
The Poisson-Lindley-Lognormal distribution is another compound distribution, but it does not have a closed form for the distribution.

Again, the Poisson-Lindley performs much better for this case with a
high number of 0 and 1 counts.

## Poisson-Lindley-Lognormal Regression

As with the Poisson-Lindley-Gamma, the Poisson-Lindley distribution can
be compounded with a Lognormal distribution. In this case, the
logarithm-transformed distribution has a mean value of 0 and a standard
deviation of $\sigma$. The density function for this is:

$$f(y|\mu,\theta,\alpha)=\int_0^\infty \frac{\theta^2\mu^y x^y(\theta+\mu\cdot x+y+1)}{(\theta+1)(\theta+\mu\cdot x)^{y+2}} \frac{exp\left(-\frac{ln^2(x)}{2\sigma^2} \right)}{x\sigma\sqrt{2\pi}}dx$$

This integral does not have a closed-form solution. Thus, this
distribution relies on simulation-based integration or quadrature-based
methods when applied to maximum likelihood estimation.

Looking at the shape of the distribution:

```{r}
pld.prob <- function(y, mu, theta){
  p <- (theta^2*mu^y*(theta+mu+y+1))/((theta+1)*(mu+theta)^(y+2))
  return(p)
}
  
# define function for adjustment to mean predictions
PLL.mean.adjustment <- function(theta, sigma){
  return((theta+2)/(theta*(theta+1)))*exp(sigma^2/2)
}

pll.prob <- function(Y, mu, sigma, theta, ndraws=1500) {
  
  # Validate input
  if(length(Y) != length(mu)) {
    mu <- rep(mu, length(Y))
  }
  
  library(randtoolbox)
  library(stats)
  
  # Generate Halton draws to use as quantile values
  h <- halton(ndraws)
  
  # Evaluate the density of the normal distribution at those quantiles and use the exponent to transform to lognormal values
  lnormdist <- exp(qnorm(h, 0, sigma))
  
  # Function for Poisson-Lindley Probability
  pois.lind.p <- function(y, mu, theta){
    return((theta^2 * mu^y * (theta + mu + y + 1)) / ((theta + 1) * (mu + theta)^(y + 2)))
  }
  
  # Function to compute Poisson-Gamma-Lindley for a single y and mu
  compute_pgl <- function(y, mu) {
    mu_i <- outer(mu, lnormdist)
    
    # Compute Poisson-Lindley for given y and mu
    p_plind.i <- apply(mu_i, 1, function(x) pois.lind.p(y, mu=x, theta=theta))
    
    # Multiply Poisson-Lindley values by the gamma density values
    product_vals <- rowMeans(p_plind.i)
    
    # Return the mean of product values
    return(mean(product_vals))
  }
  
  # Apply compute_pgl function to each element of Y and mu
  results <- mapply(compute_pgl, y=Y, mu=mu)
  
  return(results)
}

# Plot the function
library(ggplot2)

plplots1 <- function(theta, sigma, max=20){
  
  x <- seq(from=0,to=max,by=1)
  
  adjust <- PLL.mean.adjustment(theta, sigma)

  df <- data.frame(x)
  
  df$theta05005 <- pll.prob(x, mu=0.05/adjust, sigma=sigma, theta=theta)
  df$theta05025 <- pll.prob(x, mu=0.25/adjust, sigma=sigma, theta=theta)
  df$theta0505 <- pll.prob(x, mu=0.5/adjust, sigma=sigma, theta=theta)
  df$theta051 <- pll.prob(x, mu=1/adjust, sigma=sigma, theta=theta)
  df$theta052 <- pll.prob(x, mu=2/adjust, sigma=sigma, theta=theta)
  df$theta053 <- pll.prob(x, mu=3/adjust, sigma=sigma, theta=theta)
  df$theta054 <- pll.prob(x, mu=4/adjust, sigma=sigma, theta=theta)
  df$theta055 <- pll.prob(x, mu=5/adjust, sigma=sigma, theta=theta)
  df$theta0510 <- pll.prob(x, mu=10/adjust, sigma=sigma, theta=theta)
  
  
  pld05 <- ggplot(df)+
    geom_line(aes(y=theta05005, x=x, color='Mean=0.05'))+
    geom_line(aes(y=theta05025, x=x, color='Mean=0.25'))+
    geom_line(aes(y=theta0505, x=x, color='Mean=0.5'))+
    geom_line(aes(y=theta051, x=x, color='Mean=1'))+
    geom_line(aes(y=theta052, x=x, color='Mean=2'))+
    geom_line(aes(y=theta053, x=x, color='Mean=3'))+
    geom_line(aes(y=theta054, x=x, color='Mean=4'))+
    geom_line(aes(y=theta055, x=x, color='Mean=5'))+
    #geom_line(aes(y=theta0510, x=x, color='Mean=10'))+
    labs(title=paste("Poisson-Lindley-Lognormal \n For theta = ", theta, "\n And sigma = ",sigma),
         y="Probability",
         x='Integer Value') + 
            theme_bw()
  print(pld05)
  return(pld05)
}

plplots1(theta=0.05, sigma=0.5)

```

```{r}
plplots1(theta=0.05, sigma=1)
```

```{r}
plplots1(theta=0.25, sigma=5)
```

```{r}
plplots1(theta=0.25, sigma=1)
```

```{r}
plplots1(theta=3, sigma=1)
```

```{r}
plplots1(theta=5, sigma=1)
```

```{r}
plplots1(theta=10, sigma=0.5)
```

## Poisson-Lindley-Gamma (i.e., Negative-Binomial-Lindley)


```{r}
plg.prob <- function(Y, mu, alpha, theta, ndraws=1500) {
  
  # Validate input
  if(length(Y) != length(mu)) {
    mu <- rep(mu, length(Y))
  }
  
  library(randtoolbox)
  library(stats)
  
  # Generate Halton draws to use as quantile values
  h <- halton(ndraws)
  
  # Evaluate the density of the gamma distribution at those quantiles 
  gammadist <- qgamma(h, alpha, rate=alpha)
  
  # Function for Poisson-Lindley Probability
  pois.lind.p <- function(y, mu, theta){
    return((theta^2 * mu^y * (theta + mu + y + 1)) / ((theta + 1) * (mu + theta)^(y + 2)))
  }
  
  # Function to compute Poisson-Gamma-Lindley
  compute_pgl <- function(y, mu) {
    mu_i <- outer(mu, gammadist)
    
    # Compute Poisson-Lindley for given y and mu
    p_plind.i <- apply(mu_i, 1, function(x) pois.lind.p(y, mu=x, theta=theta))
    
    # Multiply Poisson-Lindley values by the gamma density values
    product_vals <- rowMeans(p_plind.i)
    
    # Return the mean of product values
    return(mean(product_vals))
  }
  
  # Apply compute_pgl function to each element of Y and mu
  results <- mapply(compute_pgl, y=Y, mu=mu)
  
  return(results)
}


plplots1 <- function(theta, alpha, max=20){
  
  x <- seq(from=0,to=max,by=1)
  
  adjust <- (theta+2)/(theta*(theta+1))

  df <- data.frame(x)
  
  df$theta05005 <- plg.prob(x, mu=0.05/adjust, alpha=alpha, theta=theta)
  df$theta05025 <- plg.prob(x, mu=0.25/adjust, alpha=alpha, theta=theta)
  df$theta0505 <- plg.prob(x, mu=0.5/adjust, alpha=alpha, theta=theta)
  df$theta051 <- plg.prob(x, mu=1/adjust, alpha=alpha, theta=theta)
  df$theta052 <- plg.prob(x, mu=2/adjust, alpha=alpha, theta=theta)
  df$theta053 <- plg.prob(x, mu=3/adjust, alpha=alpha, theta=theta)
  df$theta054 <- plg.prob(x, mu=4/adjust, alpha=alpha, theta=theta)
  df$theta055 <- plg.prob(x, mu=5/adjust, alpha=alpha, theta=theta)
  df$theta0510 <- plg.prob(x, mu=10/adjust, alpha=alpha, theta=theta)
  
  
  pld05 <- ggplot(df)+
    geom_line(aes(y=theta05005, x=x, color='Mean=0.05'))+
    geom_line(aes(y=theta05025, x=x, color='Mean=0.25'))+
    geom_line(aes(y=theta0505, x=x, color='Mean=0.5'))+
    geom_line(aes(y=theta051, x=x, color='Mean=1'))+
    geom_line(aes(y=theta052, x=x, color='Mean=2'))+
    geom_line(aes(y=theta053, x=x, color='Mean=3'))+
    geom_line(aes(y=theta054, x=x, color='Mean=4'))+
    geom_line(aes(y=theta055, x=x, color='Mean=5'))+
    #geom_line(aes(y=theta0510, x=x, color='Mean=10'))+
    labs(title=paste("Poisson-Lindley-Gamma \n For theta = ", theta, "\n And alpha = ",alpha),
         y="Probability",
         x='Integer Value') + 
            theme_bw()
  print(pld05)
  return(pld05)
}

plplots1(theta=0.05, alpha=0.5)

```

```{r}
plplots1(theta=0.05, alpha=1)
```

```{r}
plplots1(theta=1, alpha=12)
```

```{r}
plplots1(theta=0.25, alpha=1)
```

```{r}
plplots1(theta=3, alpha=1)
```

```{r}
plplots1(theta=10, alpha=10)
```

```{r}
plplots1(theta=10, alpha=0.5)
```


Halton-Draw-based integration is used in the functions below.

```{r Poisson-Lindley Lognormal Regression}

library(MASS)
library(nlme) # use for formatting returned model
library(maxLik)
library(pracma)

pll.reg <- function(formula, data, method = 'BHHH', max.iters = 200) { 
  # This function estimates a Poisson-Lindley-Lognormal mixture model
  # The estimation use Guass-hermite quadrature
  # The formula follows standard R syntax
  # The data input is a dataframe that the model should be estimated on
  # method can take any estimation method allowed in the maxLik library
  # max.iters is the maximum number of iterations allowed in the optimization method
  
  mod_df <- model.frame(formula, data)
  X <- model.matrix(formula, data)
  y <- as.numeric(model.response(mod_df))
  x_names <- Names(formula, data)
  
  p_model <- glm.nb(formula, data)
  start <- as.vector(p_model$coefficients)
  s <- 0.05
  lntheta <- 1.9 # starting value for ln(theta)
  start <- append(start, s) # Add starting values for sigma
  start <- append(start, lntheta) # Add starting values for ln(theta)
  
  pll.prob <- function(Y, mu, sigma, theta, ndraws=1500) {
  
    # Validate input
    if(length(Y) != length(mu)) {
      mu <- rep(mu, length(Y))
    }
    
    library(randtoolbox)
    library(stats)
    
    # Generate Halton draws to use as quantile values
    h <- halton(ndraws)
    
    # Evaluate the density of the normal distribution at those quantiles and use the exponent to transform to lognormal values
    lnormdist <- exp(qnorm(h, 0, sigma))
    
    # Function for Poisson-Lindley Probability
    pois.lind.p <- function(y, mu, theta){
      return((theta^2 * mu^y * (theta + mu + y + 1)) / ((theta + 1) * (mu + theta)^(y + 2)))
    }
    
    # Function to compute Poisson-Gamma-Lindley for a single y and mu
    compute_pgl <- function(y, mu) {
      mu_i <- outer(mu, lnormdist)
      
      # Compute Poisson-Lindley for given y and mu
      p_plind.i <- apply(mu_i, 1, function(x) pois.lind.p(y, mu=x, theta=theta))
      
      # Multiply Poisson-Lindley values by the gamma density values
      product_vals <- rowMeans(p_plind.i)
      
      # Return the mean of product values
      return(mean(product_vals))
    }
    
    # Apply compute_pgl function to each element of Y and mu
    results <- mapply(compute_pgl, y=Y, mu=mu)
    
    return(results)
  }
  
  # define function for adjustment to mean predictions
  PLD.mean.adjustment <- function(theta){
    return((theta+2)/(theta*(theta+1)))
  }
  start[1] <- start[1] - log(PLD.mean.adjustment(exp(lntheta))) - s^2/2 # adjust starting value of intercept for theta
  
  p_poisson_lindley_lognormal <- function(p, y, X, est_method){
    numcoefs <- length(p)-2
    coefs <- p[1:numcoefs]
    sigma <- abs(p[(length(p)-1)])
    theta <- exp(p[length(p)])
    mu <- exp(X %*% coefs)
    probs <- pll.prob(y, mu, sigma, theta)

    ll <- sum(log(probs))
    if (est_method == 'bhhh' | est_method == 'BHHH'){
      return(log(probs))
    } else{return(ll)}
  }
  
  fit <- maxLik(p_poisson_lindley_lognormal, 
                start = start,
                y = y,
                X = X,
                est_method = method,
                method = method,
                control = list(iterlim = max.iters, printLevel = 1))

  beta_est <- fit$estimate
  numcoefs <- length(beta_est)-2
  beta_pred <- beta_est[1:numcoefs]
  beta.params <- tail(beta_est)
  x_names <- append(x_names, 'sigma')
  x_names <- append(x_names, 'ln(theta)')
  names(fit$estimate) <- x_names
  fit$estimate['sigma'] <- abs(fit$estimate['sigma'])

  sigma_sq <- fit$estimate['sigma']^2
  theta.est <- exp(fit$estimate['ln(theta)'])

  fit$formula <- formula
  fit$sigma <- beta.params[1]
  fit$theta <- theta.est
  fit$mean.adjustment <- PLD.mean.adjustment(theta.est)
  fit$predictions <- exp(X %*% beta_pred + sigma_sq/2) * fit$mean.adjustment
  fit$observed <- y
  fit$residuals <- y - fit$predictions
  # Note that this has the predictions, residuals, and observed outcome stored with the model
  
  return(fit)
}

predict.pll <- function(model, data){
  # This function takes in a Poisson Lognormal model object and dataframe
  # The function returns a dataframe with predictions, observed outcome, and residuals for the data that was input
  
  mod_df <- model.frame(model$formula, data)
  X <- model.matrix(model$formula, data)
  y <- as.numeric(model.response(mod_df))
  
  beta_est <- model$estimate
  beta_pred <- head(beta_est, -2)
  
  sigma_sq <- model$sigma^2
  adjustment <- model$mean.adjustment
  
  predictions <- exp(X %*% beta_pred + sigma_sq/2) * adjustment
  observed <- y
  residuals <- y - predictions

  pred <- list('prediction'=predictions, 'observed'=observed, 'residuals'=residuals)
  
  return(pred)
}
```

### Now, test this on data

Note that the default test for log(theta) is if the coefficient is
different from a value of 0 (different from theta=1). This has no
practical meaning. To determine if the model is better than the Poisson,
or other count models, use the LR test, AIC, BIC, etc.

```{r}
pll.base <- pll.reg(Rollover ~ lnaadt + lnlength + speed60,
                        data=df,
                        method = 'BFGS',
                        max.iters=300)

summary(pll.base)
```

```{r}
preds <- predict.pll(pll.base, df)
```

```{r}
res <- pll.base$residuals
predictions <- pll.base$predictions
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```

Compare with NB

```{r}
nb.base <- glm.nb(Animal ~ lnaadt + lnlength + speed60,
                        data=df)

summary(nb.base)
```

```{r}
res <- nb.base$residuals
predictions <- nb.base$fitted.values
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```

Comparing fit statistics:

```{r}
library(kableExtra)
rmse <- function(res){
  return(round(sqrt(mean(res^2)),4))
}

mae <- function(res){
  return(round(mean(abs(res)),4))
}

bias <- function(res, obs){
  return(round(sum(res)/sum(obs)*100,4))
}

#Negative Binomial
nb.rmse <- rmse(nb.base$residuals)
nb.mae <- mae(nb.base$residuals)
nb.bias <- bias(nb.base$residuals, nb.base$y)

# Poisson-Lindley-Lognormal
pll.rmse <- rmse(pll.base$residuals)
pll.mae <- mae(pll.base$residuals)
pll.bias <- bias(pll.base$residuals, pll.base$observed)

# Poisson-Lindley
pl.rmse <- rmse(pl.base$residuals)
pl.mae <- mae(pl.base$residuals)
pl.bias <- bias(pl.base$residuals, pl.base$observed)

# Creating a matrix with the values
values_matrix <- matrix(c("RMSE", "MAE", "Bias (%)",
                          nb.rmse, nb.mae, nb.bias,
                          pl.rmse, pl.mae, pl.bias,
                          pll.rmse, pll.mae, pll.bias),
                        nrow = 3, ncol = 4, byrow = FALSE)

# Converting matrix to a data frame
values_df <- as.data.frame(values_matrix)

# Assigning column names
colnames(values_df) <- c("Fit Metric", "Negative Binomial", "Poisson-Lindley", "Poisson-Lindley-Lognormal")

kbl(values_df, format='simple', booktabs=TRUE)

```

From the CURE plots and the fit statistics, the Poisson-Lindley is
clearly fitting the data better due to the high number of values for 0
or 1 crashes.


## Poison-Linldey-Gamma Regression
```{r Poisson-Lindley Gamma Regression}

library(MASS)
library(nlme) # use for formatting returned model
library(maxLik)
library(pracma)

plg.reg <- function(formula, data, method = 'BHHH', max.iters = 200) { 
  # This function estimates a Poisson-Lindley-Gamma mixture model
  # The estimation use Guass-hermite quadrature
  # The formula follows standard R syntax
  # The data input is a dataframe that the model should be estimated on
  # method can take any estimation method allowed in the maxLik library
  # max.iters is the maximum number of iterations allowed in the optimization method
  
  mod_df <- model.frame(formula, data)
  X <- model.matrix(formula, data)
  y <- as.numeric(model.response(mod_df))
  x_names <- Names(formula, data)
  
  p_model <- glm.nb(formula, data)
  start <- as.vector(p_model$coefficients)
  lna <- -6 #log(1/p_model$theta)
  lntheta <- -1.9 #0 # starting value for ln(theta)
  start <- append(start, lna) # Add starting values for ln(alpha)
  start <- append(start, lntheta) # Add starting values for ln(theta)
  
  # define function for adjustment to mean predictions
  PLD.mean.adjustment <- function(theta){
    return((theta+2)/(theta*(theta+1)))
  }
  start[1] <- start[1] - log(PLD.mean.adjustment(exp(lntheta))) # adjust starting value of intercept for theta
  
  p_poisson_lindley_gamma <- function(p, y, X, est_method){
    numcoefs <- length(p)-2
    coefs <- p[1:numcoefs]
    alpha <- exp(p[(length(p)-1)])
    theta <- exp(p[length(p)])
    mu <- exp(X %*% coefs)
    probs <- plg.prob(y, mu, alpha=alpha, theta=theta)

    ll <- sum(log(probs))
    if (est_method == 'bhhh' | est_method == 'BHHH'){
      return(log(probs))
    } else{return(ll)}
  }
  
  fit <- maxLik(p_poisson_lindley_gamma, 
                start = start,
                y = y,
                X = X,
                est_method = method,
                method = method,
                control = list(iterlim = max.iters, printLevel = 2))

  beta_est <- fit$estimate
  numcoefs <- length(beta_est)-2
  beta_pred <- beta_est[1:numcoefs]
  beta.params <- tail(beta_est)
  x_names <- append(x_names, 'ln(alpha)')
  x_names <- append(x_names, 'ln(theta)')
  names(fit$estimate) <- x_names

  alpha <- exp(beta.params[1])
  theta.est <- exp(beta.params[2])

  fit$formula <- formula
  fit$sigma <- beta.params[1]
  fit$theta <- theta.est
  fit$mean.adjustment <- PLD.mean.adjustment(theta.est)
  fit$predictions <- exp(X %*% beta_pred) * fit$mean.adjustment
  fit$observed <- y
  fit$residuals <- y - fit$predictions
  # Note that this has the predictions, residuals, and observed outcome stored with the model
  
  return(fit)
}

predict.plg <- function(model, data){
  # This function takes in a Poisson Lognormal model object and dataframe
  # The function returns a dataframe with predictions, observed outcome, and residuals for the data that was input
  
  mod_df <- model.frame(model$formula, data)
  X <- model.matrix(model$formula, data)
  y <- as.numeric(model.response(mod_df))
  
  beta_est <- model$estimate
  beta_pred <- head(beta_est, -2)
  theta <- model$theta
  
  predictions <- exp(X %*% beta_pred) * (theta+2)/(theta*(theta+1))
  observed <- y
  residuals <- y - predictions

  pred <- list('prediction'=predictions, 'observed'=observed, 'residuals'=residuals)
  
  return(pred)
}
```

### Now, test this on data

Note that the default test for log(theta) is if the coefficient is
different from a value of 0 (different from theta=1). This has no
practical meaning. To determine if the model is better than the Poisson,
or other count models, use the LR test, AIC, BIC, etc.

```{r}
pll.base <- plg.reg(Rollover ~ lnaadt + lnlength + speed60,
                        data=df,
                        method = 'NM',
                        max.iters=300)

summary(pll.base)
```

```{r}
preds <- predict.plg(pll.base, df)
```

```{r}
res <- pll.base$residuals
predictions <- pll.base$predictions
cure_df <- calculate_cure_dataframe(predictions, res)
cure_plot(cure_df)
```
